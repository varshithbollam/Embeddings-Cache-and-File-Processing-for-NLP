{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sqt1CWtmXkCp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "4ffc5abc-8a24-4e3b-8421-7d5db4d7d7be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'compute_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-75641f6dd39b>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# First call: Compute embeddings for prompt_1 and cache the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0membeddings_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_embeddings_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Second call: Retrieve embeddings for prompt_1 from cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-75641f6dd39b>\u001b[0m in \u001b[0;36mcompute_embeddings_with_cache\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# If prompt not in cache, perform computation (e.g., generate embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Store computed embeddings in cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'compute_embeddings' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize an empty cache (dictionary)\n",
        "cache = {}\n",
        "\n",
        "# Function to compute embeddings for a prompt with caching\n",
        "def compute_embeddings_with_cache(prompt):\n",
        "    if prompt in cache:\n",
        "        # If prompt exists in cache, return cached embeddings\n",
        "        return cache[prompt]\n",
        "    else:\n",
        "        # If prompt not in cache, perform computation (e.g., generate embeddings)\n",
        "        embeddings = compute_embeddings(prompt)\n",
        "\n",
        "        # Store computed embeddings in cache\n",
        "        cache[prompt] = embeddings\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "# Example usage\n",
        "prompt_1 = \"This is a sample prompt.\"\n",
        "prompt_2 = \"Another prompt.\"\n",
        "\n",
        "# First call: Compute embeddings for prompt_1 and cache the result\n",
        "embeddings_1 = compute_embeddings_with_cache(prompt_1)\n",
        "\n",
        "# Second call: Retrieve embeddings for prompt_1 from cache\n",
        "cached_embeddings_1 = compute_embeddings_with_cache(prompt_1)\n",
        "\n",
        "# Third call: Compute embeddings for prompt_2 and cache the result\n",
        "embeddings_2 = compute_embeddings_with_cache(prompt_2)\n",
        "\n",
        "# Fourth call: Retrieve embeddings for prompt_2 from cache\n",
        "cached_embeddings_2 = compute_embeddings_with_cache(prompt_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import PyPDF2\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "model_name = \"bert-base-uncased\"  # Example: BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Function to preprocess and generate embeddings for a file\n",
        "def process_and_generate_embeddings(file_path):\n",
        "    # Extract text from the file\n",
        "    if file_path.endswith('.pdf'):\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "    else:\n",
        "        # Handle other file types\n",
        "        text = \"\"\n",
        "\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Forward pass through model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract embeddings\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
        "        for page_num in range(pdf_reader.numPages):\n",
        "            page = pdf_reader.getPage(page_num)\n",
        "            text += page.extractText()\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "file_path = \"example.pdf\"\n",
        "embeddings = process_and_generate_embeddings(file_path)\n",
        "# Upload embeddings to LLM (implementation not provided)\n"
      ],
      "metadata": {
        "id": "qiXZ5iBzX2vq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}